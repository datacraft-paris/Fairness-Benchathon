{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAIRNESS BENCHATON\n",
    "\n",
    "_________________\n",
    "\n",
    "This notebook compiles the results of the Fairness Benchaton conducted in june 2021 at datacraft. \n",
    "\n",
    "Currently results have been merged from : Aequitas and Fairlearn\n",
    "\n",
    "<u>Methodology:</u>\n",
    "\n",
    "We used the dataset about <a href='https://raw.githubusercontent.com/ekimetrics/ethical-ai-toolkit/main/data/german-risk-scoring.csv'>credit risk </a> scoring categorizing german individuals by various socio-economical attributes (gender,age,family, incomes...)  and annotated with the risk associated with each individuals as a credit carrier (high risk or low risk).<br>\n",
    "This annotation is directly the judgment of a business expert which means it is not based on a ground truth (like a retroactive annotation after the fact) but rather purely reflects the mechanism of choice in the fields. <br>\n",
    "This leads to the non-surprising effect that the model trained on this dataset tends to be biaised toward certain features such as age and gender to name a few , effectively realizing a discrimination. \n",
    "\n",
    "<u> Structure </u>\n",
    "\n",
    "The idea is to layout the various benchmark using :\n",
    "    * the same training data\n",
    "    * the same models. \n",
    "\n",
    "1. <u>the common ground:</u> imports , class definition and such... all the titles of this section are black\n",
    "\n",
    "2. <u style='color:green'>the load of the data:</u> , analysis and the training of two reference model for the benchmark.\n",
    "\n",
    "2. <u style='color:brown'>Aequitas:</u> benchmark of Aequitas librairy , all the titles of this section are brown\n",
    "\n",
    "3. <u style='color:blue'>Fairlearn:</u> benchmark of Fairlearn librairy , all the titles of this section are blue. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first cells are the 'common ground' where the data are loaded , analyzed and two models are trained on the training data,   a logistic regression model and a random forest.<br>\n",
    "Both model information (y_predict , accuracy, training data...) are encapsulated into two object-variables: \n",
    "    <span style='color:green'> logistic_regression</span> and <span style='color:green'> random_forest </span>\n",
    "    the training data are also stored in the object-variable  <span style='color:green'> training_data </span>\n",
    "\n",
    "Two classes have been defined : \n",
    "    * ModelRun to store all the information about a model into a single variable\n",
    "    * SplitData to store all the information about the training data into a single variable. \n",
    "\n",
    "This simplifies the chaining of benchmarks and opens the way for a possible comparison and cross-usage of the libs. \n",
    "\n",
    "<u style='color:green'>Details of the classes</u> \n",
    "\n",
    "The class ModelRun stores the information using the following structure: \n",
    "* training_data : is a SplitData Object containing X_train, X_test, Y_train....\n",
    "* model : the model used (a scikit-learn Object) \n",
    "* y_predict : the predictions\n",
    "* accuracy : the accuracy score\n",
    "* confusion matrix : the confusion matrix\n",
    "\n",
    "The class SplitData is just an encapsulation of the sklearn train_test_split() for the sake of storing variables in case of multiple training data (currently overkill but it makes the previous class a bit simpler). It's attributes are (logically) X_train,X_test,y_train and y_test. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "print('done importing modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,X_train,y_train,X_test,remove_warnings=True,display_result=True):\n",
    "    if remove_warnings:\n",
    "        warnings.filterwarnings('ignore')\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    matrix = confusion_matrix(y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    plt.title(f\"{str(model)} - accuracy {acc:.4f}\")\n",
    "    sns.heatmap(matrix,annot = True,cmap = \"Blues\",square = True)\n",
    "    plt.show()\n",
    "    \n",
    "    return y_test,y_pred,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        stratify=None\n",
    "    ):\n",
    "        stratify=y if stratify is None else stratify\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(X,y,test_size = test_size,stratify=y)\n",
    "   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRun:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        X=None,\n",
    "        y=None,\n",
    "        training_data=None,              \n",
    "        remove_warnings=True\n",
    "    ):\n",
    "        if not any([var is not None for var in [X,training_data]]):\n",
    "            raise Exception('you must provide either training data as a SplitData() object  or X,y arguments ')\n",
    "        self.training_data = SplitData(X,y,test_size = 0.3,stratify=y) if training_data is None else training_data\n",
    "        self.model=model\n",
    "        \n",
    "        # registering training results attributes\n",
    "        self.y_pred=None,\n",
    "        self.accuracy=None,\n",
    "        self.confusion_matrix=None     \n",
    "    \n",
    "    def train(self,remove_warnings=False,display_result=True):\n",
    "        if remove_warnings:\n",
    "            warnings.filterwarnings('ignore')  # at your own risk :)\n",
    "        self.model.fit(self.training_data.X_train,self.training_data.y_train)\n",
    "        self.y_pred=self.model.predict(self.training_data.X_test) \n",
    "        self.accuracy=accuracy_score(self.training_data.y_test,self.y_pred)\n",
    "        self.confusion_matrix=confusion_matrix(self.training_data.y_test,self.y_pred)\n",
    "        if display_result:\n",
    "            self.draw_result()\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def draw_result(self):\n",
    "        plt.title(f\"{str(self.model)} - accuracy {self.accuracy:.4f}\")\n",
    "        sns.heatmap(self.confusion_matrix,annot = True,cmap = \"Blues\",square = True)\n",
    "        plt.show()\n",
    "        return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style='color:green'>Data preparation and model training\n",
    "##  <span style='color:green'>Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/ekimetrics/ethical-ai-toolkit/main/data/german-risk-scoring.csv\")\n",
    "target = 'Cost Matrix(Risk)'\n",
    "\n",
    "data[\"sex\"] = data[\"Personal status and sex\"].map(lambda x : x.split(\":\")[0])\n",
    "data[target]=data[target].replace({'Good Risk': 1, 'Bad Risk': 0})\n",
    "data = data.rename(columns = {\"Age in years\":\"age\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style='color:green'>quick analysis of dependencies toward target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if is_numeric_dtype(data[col]):\n",
    "        # Magic dist function by seaborn\n",
    "        plt.figure(figsize = (15,3))\n",
    "        plt.title(col)\n",
    "        sns.histplot(data = data,x = col,hue = target,multiple=\"stack\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Magic catplot function by seaborn\n",
    "        plt.figure(figsize = (15,3))\n",
    "        plt.title(col)\n",
    "        sns.countplot(data = data,x = col,hue = target)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style='color:green'>recording columns by data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'Status of existing checking account',\n",
    "    'Credit history',\n",
    "    'Purpose',\n",
    "    'Savings account/bonds',\n",
    "    'Present employment since',\n",
    "    'Personal status and sex', \n",
    "    'Other debtors / guarantors',\n",
    "    'Property',\n",
    "    'Other installment plans',\n",
    "    'Housing',\n",
    "    'Job',\n",
    "    'Telephone',\n",
    "    'foreign worker',\n",
    "    'sex'\n",
    "       ]\n",
    "numericals = [\n",
    "    'Duration in month',\n",
    "    'Credit amount',\n",
    "    'Installment rate in percentage of disposable income',\n",
    "    'Present residence since',\n",
    "    'age',\n",
    "    'Number of existing credits at this bank',\n",
    "    'Number of people being liable to provide maintenance for',\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style='color:green'>train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([\n",
    "    pd.get_dummies(data[categoricals]),\n",
    "    data[numericals],\n",
    "],axis = 1)\n",
    "X = X.drop(columns = [\"sex_male\"])\n",
    "y = data[target]\n",
    "X.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=SplitData(X=X,y=y)\n",
    "training_data.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest=ModelRun(\n",
    "    model=RandomForestClassifier(random_state=1),\n",
    "    training_data=training_data\n",
    ")\n",
    "random_forest.train()\n",
    "random_forest.y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression=ModelRun(\n",
    "    model=LogisticRegression(),\n",
    "    training_data=training_data\n",
    ")\n",
    "logistic_regression.train(remove_warnings=True)\n",
    "logistic_regression.y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know have two objects representing our baseline trained models : <span style='color:green'>logistic_regression </span>and <span style='color:green'>random_forest</span> that we can refer to in the benchmark section. Those object contains their training data (common) refered to as <span style='color:green'>training_data</span>  (either directly or through the modelRun.training_data attribute). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style='color:red'>Analysis using benched libs\n",
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding some meta information about data structure.\n",
    "_____________\n",
    "\n",
    "one last step before we go ,to use the libs it's better to have stored the name of categorical and numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_to_analyse = [\n",
    "#     'Status of existing checking account',\n",
    "#     'Credit history',\n",
    "#     'Purpose',\n",
    "#     'Savings account/bonds',\n",
    "#     'Present employment since',\n",
    "#     'Personal status and sex', \n",
    "#     'Other debtors / guarantors',\n",
    "#     'Property',\n",
    "#     'Other installment plans',\n",
    "#     'Housing',\n",
    "#     'Job',\n",
    "#     'Telephone',\n",
    "#     'foreign worker'\n",
    "    'sex_female'\n",
    "       ]\n",
    "\n",
    "numericals_to_analyse = [\n",
    "#     'Duration in month',\n",
    "     'Credit amount',\n",
    "#     'Installment rate in percentage of disposable income',\n",
    "#     'Present residence since',\n",
    "     'age'#,\n",
    "#     'Number of existing credits at this bank',\n",
    "#     'Number of people being liable to provide maintenance for',\n",
    "]\n",
    "\n",
    "fields_to_analyse = cats_to_analyse + numericals_to_analyse\n",
    "analysis = training_data.X_test[fields_to_analyse].copy()\n",
    "analysis['score'] = logistic_regression.y_pred\n",
    "analysis['label_value'] = training_data.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:brown'> AEQUITAS </span>\n",
    "________________\n",
    "\n",
    "http://www.datasciencepublicpolicy.org/projects/aequitas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning, AI and Data Science based predictive tools are being increasingly used in problems that can have a drastic impact on people’s lives in policy areas such as criminal justice, education, public health, workforce development and social services. Recent work has raised concerns on the risk of unintended bias in these models, affecting individuals from certain groups unfairly. While a lot of bias metrics and fairness definitions have been proposed, there is no consensus on which definitions and metrics should be used in practice to evaluate and audit these systems. Further, there has been very little empirical work done on using and evaluating these measures on real-world problems, especially in public policy.\n",
    "\n",
    "Aequitas, an open source bias audit toolkit developed by the Center for Data Science and Public Policy at University of Chicago, can be used to audit the predictions of machine learning based risk assessment tools  to understand different types of biases, and make informed decisions about developing and deploying such systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'> basics of AEQUITAS:\n",
    "    \n",
    "Aequitas is meant to audit your dataset using four metrics that are supposed to describe 'how biaised' are your system and which \"category of impacts\" your can expect:\n",
    "\n",
    "1. Equal parity : <br>\n",
    "If you want each group to be represented equally among the selected set\n",
    "\n",
    "2. Proportional parity: <br>\n",
    "If you want each group represented proportional to their representation in the overall population\n",
    "\n",
    "3. False Positive Parity:<br>\n",
    "If you want each group to have equal False Positive Rates\n",
    "\n",
    "4. False Negative Parity:<br>\n",
    "If you want each group to have equal False Negative Rates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://www.datasciencepublicpolicy.org/wp-content/uploads/2021/04/Fairness-Short-Tree-1200x746.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'> metrics in code\n",
    "\n",
    "https://dssg.github.io/aequitas/\n",
    "    \n",
    "<img src=\"https://dssg.github.io/aequitas/_images/preliminary_concepts.jpg\">\n",
    "<img src=\"https://dssg.github.io/aequitas/_images/metrics.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'> tweaking data : restoring the 'sex' column\n",
    "___________\n",
    "the preprocessing of aequitas that we used do not handle very well booleans (the one we used probably isn't the 'right'way) so...back to a categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis['sex']=analysis.sex_female.apply(lambda x:'female' if x else 'male')\n",
    "cats_to_analyse.append('sex')\n",
    "cats_to_analyse.remove('sex_female')\n",
    "analysis.drop('sex_female',axis=1,inplace=True)\n",
    "analysis.sample(n=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'> using aequitas preprocessing\n",
    "_______\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.preprocessing import preprocess_input_df\n",
    "df, _ = preprocess_input_df(analysis)\n",
    "df.sample(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import karmahutils as kut\n",
    "for col in df.columns:\n",
    "    kut.display_message(col)\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dssg.github.io/aequitas/api/aequitas.html#src.aequitas.preprocessing.preprocess_input_df\">preprocess_input_df</a> is basically a way to assign cohort belonging to the instances of your dataset. Each row will then be described by a discretize range of value instead of column values. \n",
    "basically implements <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html\">pandas.qcut</a> based on quartiles. A huge drawback is that you can not customize the number of bins that you want. It's a break in four unless your cardinality is lower. And that's it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.group import Group\n",
    "g = Group()\n",
    "xtab, _ = g.get_crosstabs(df)\n",
    "xtab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dssg.github.io/aequitas/api/aequitas.html#src.aequitas.group.Group.get_crosstabs\"> get_crosstabs</a> basically computes the metric for each basic cohort resulting from the grouping in quartiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'> plotting the audit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.plotting import Plot\n",
    "    \n",
    "plt.figure(figsize = (25, 15))\n",
    "aqp = Plot()\n",
    "fpr_plot = aqp.plot_group_metric_all(xtab, metrics=['ppr','pprev','fnr','fpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_plot = aqp.plot_group_metric(xtab, 'fpr', min_group_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.bias import Bias\n",
    "group_dict = {\n",
    "    'age':df['age'].unique()[1], \n",
    "    'Credit amount':df['Credit amount'].unique()[1],\n",
    "    'sex' : 'female'\n",
    "}\n",
    "b = Bias()\n",
    "bdf = b.get_disparity_predefined_groups(xtab, \n",
    "                    original_df=df, \n",
    "                    ref_groups_dict=group_dict, \n",
    "                    alpha=0.05, \n",
    "                    check_significance=False)\n",
    "print('analyzing group')\n",
    "print(group_dict)\n",
    "bdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib = 'sex'\n",
    "fpr_disparity = aqp.plot_disparity(\n",
    "    bdf,\n",
    "    group_metric='fpr_disparity',\n",
    "    attribute_name=attrib\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib = 'age'\n",
    "fpr_disparity = aqp.plot_disparity(\n",
    "    bdf,\n",
    "    group_metric='fpr_disparity',\n",
    "    attribute_name=attrib\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib = 'Credit amount'\n",
    "fpr_disparity = aqp.plot_disparity(bdf, group_metric='fpr_disparity', \n",
    "                                       attribute_name=attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.fairness import Fairness\n",
    "    \n",
    "f = Fairness()\n",
    "fdf = f.get_group_value_fairness(bdf)\n",
    "fpr_fairness = aqp.plot_fairness_group(fdf, group_metric='fpr', title=True, min_group_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>FAIRLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate\n",
    "# from fairlearn.widget import FairlearnDashboard   << removed to be replaced by raiwidgets\n",
    "from raiwidgets import FairnessDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since fairlearn 0.7, FairLearnDashboard class do not exists. Instead it's another dedicated microsoft project that took over (raiwidgets). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> Auditing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MetricFrame(accuracy_score,training_data.y_test, logistic_regression.y_pred, sensitive_features=training_data.X_test[\"sex_female\"])\n",
    "print(mf.overall)\n",
    "print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MetricFrame(selection_rate, training_data.y_test, random_forest.y_pred, sensitive_features=training_data.X_test[\"sex_female\"])\n",
    "print(mf.overall)\n",
    "print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import FairnessDashboard\n",
    "# A_test contains your sensitive features (e.g., age, binary gender)\n",
    "# y_true contains ground truth labels\n",
    "# y_pred contains prediction labels\n",
    "dashboard=FairnessDashboard(\n",
    "    sensitive_features=training_data.X_test,\n",
    "    y_true=training_data.y_test,\n",
    "    y_pred={\"initial model\": logistic_regression.y_pred}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> Correcting the biais\n",
    "\n",
    "__________________\n",
    "\n",
    "Fairlearn implements a possible 'correction' to biaised dataset by increasing the weight of a marginal cohort by a factor that is proportional to the invert of its ratio in the dataset (link needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:blue'> recreating an unmitigated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=X['sex_female']\n",
    "X_=X.drop(labels=['sex_female'], axis=1)\n",
    "X_train,X_test,y_train,y_test, A_train, A_test = train_test_split(X_,y,A, test_size = 0.3,stratify=data[target])\n",
    "unmitigated_predictor = LogisticRegression()\n",
    "unmitigated_predictor.fit(X_train, y_train)\n",
    "FairnessDashboard(\n",
    "    sensitive_features=A_test,\n",
    "    y_true=y_test,\n",
    "    y_pred={\"unmitigated\": unmitigated_predictor.predict(X_test)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> using fairlearn alternate 'less_biaised' training tools\n",
    "__________\n",
    "\n",
    "### <span style='color:blue'> grid search\n",
    "\n",
    "this grid search method is based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep = GridSearch(LogisticRegression(),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=100)\n",
    "\n",
    "sweep.fit(X_train, y_train,\n",
    "          sensitive_features=A_train)\n",
    "\n",
    "predictors = sweep.predictors_\n",
    "\n",
    "errors, disparities = [], []\n",
    "for m in predictors:\n",
    "    def classifier(X): return m.predict(X)\n",
    "\n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_train, pd.Series(y_train), sensitive_features=A_train)\n",
    "    disparity = DemographicParity()\n",
    "    disparity.load_data(X_train, pd.Series(y_train), sensitive_features=A_train)\n",
    "\n",
    "    errors.append(error.gamma(classifier)[0])\n",
    "    disparities.append(disparity.gamma(classifier).max())\n",
    "\n",
    "all_results = pd.DataFrame({\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\n",
    "\n",
    "non_dominated = []\n",
    "for row in all_results.itertuples():\n",
    "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"] <= row.disparity]\n",
    "    if row.error <= errors_for_lower_or_eq_disparity.min():\n",
    "        non_dominated.append(row.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_predicted = {\"unmitigated\": unmitigated_predictor.predict(X_test)}\n",
    "for i in range(len(non_dominated)):\n",
    "    key = \"dominant_model_{0}\".format(i)\n",
    "    value = non_dominated[i].predict(X_test)\n",
    "    dashboard_predicted[key] = value\n",
    "\n",
    "\n",
    "FairnessDashboard(\n",
    "    sensitive_features=A_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=dashboard_predicted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding weights to the positive \"female\" samples has increased the overall accuracy and decreased the disparity in predictions"
   ]
  }
 ],
 "metadata": {
  "creator": "Karanshade",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env DI3)",
   "language": "python",
   "name": "py-dku-venv-di3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}